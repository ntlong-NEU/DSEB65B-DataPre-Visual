{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6: Data Cleaning\n",
    "\n",
    "This is a lesson on data cleaning techniques, a crucial step in any Data Science and Machine Learning project. We will go through the most common issues in raw data and how to handle them effectively using Python, based on examples from the textbook.\n",
    "\n",
    "**Main Content:**\n",
    "1.  **Basic Data Cleaning:** Handling useless columns and duplicate data.\n",
    "2.  **Handling Outliers:** Identifying and removing abnormal values.\n",
    "3.  **Handling Missing Data:** Strategies from simple to complex for dealing with missing values.\n",
    "4.  **Guide to Using Pipelines:** Encapsulating the workflow for automation and avoiding data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Data Cleaning (Chapter 5)\n",
    "\n",
    "We will use a simplified version of the `oil-spill` dataset for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial oil-spill data:\n",
      "   f_1   f_2  f_3  f_4  f_5\n",
      "0    1  25.4  3.8    0   10\n",
      "1    2  22.3  4.1    0   12\n",
      "2    3  26.1  3.7    0   10\n",
      "3    4  24.8  3.9    0   11\n",
      "4    2  22.3  4.1    0   12\n"
     ]
    }
   ],
   "source": [
    "# Create sample oil-spill data\n",
    "csv_data = '''f_1,f_2,f_3,f_4,f_5\n",
    "1,25.4,3.8,0,10\n",
    "2,22.3,4.1,0,12\n",
    "3,26.1,3.7,0,10\n",
    "4,24.8,3.9,0,11\n",
    "2,22.3,4.1,0,12''' # Duplicate row\n",
    "\n",
    "df_oil = pd.read_csv(io.StringIO(csv_data))\n",
    "print(\"Initial oil-spill data:\")\n",
    "print(df_oil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Remove Zero-Variance Columns\n",
    "\n",
    "**Mathematical Explanation:**\n",
    "The variance of a variable `X` measures the spread of its values around the mean (μ). The formula is:\n",
    "$$ Var(X) = \\frac{\\sum_{i=1}^{N}(x_i - \\mu)^2}{N} $$\n",
    "If the variance is 0, it means that every value $x_i$ is equal to the mean μ. In other words, all values in that column are identical, and the column provides no information to distinguish between samples.\n",
    "\n",
    "--- \n",
    "### Details on `sklearn.feature_selection.VarianceThreshold`\n",
    "This is a transformer class in Scikit-learn used as a basic feature selection step.\n",
    "\n",
    "**Usage:**\n",
    "1. Initialize a `VarianceThreshold` object with the desired `threshold`.\n",
    "2. Use the `.fit_transform()` method on the data to both learn (calculate variance from the data) and transform (remove columns that don't meet the threshold).\n",
    "\n",
    "**Important Parameters:**\n",
    "- `threshold` (float, default=0.0): The variance threshold. Any feature with a variance less than or equal to this threshold will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required class\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial data:\n",
      "   f_1   f_2  f_3  f_4  f_5\n",
      "0    1  25.4  3.8    0   10\n",
      "1    2  22.3  4.1    0   12\n",
      "2    3  26.1  3.7    0   10\n",
      "3    4  24.8  3.9    0   11\n",
      "4    2  22.3  4.1    0   12\n",
      "\n",
      "Data after removing zero-variance column (f_4):\n",
      "   f_1   f_2  f_3   f_5\n",
      "0  1.0  25.4  3.8  10.0\n",
      "1  2.0  22.3  4.1  12.0\n",
      "2  3.0  26.1  3.7  10.0\n",
      "3  4.0  24.8  3.9  11.0\n",
      "4  2.0  22.3  4.1  12.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial data:\")\n",
    "print(df_oil)\n",
    "\n",
    "# Use VarianceThreshold to remove columns with zero variance\n",
    "transformer = VarianceThreshold(threshold=0)\n",
    "# Note: VarianceThreshold only works on numerical data\n",
    "data_transformed = transformer.fit_transform(df_oil)\n",
    "\n",
    "# Get the names of the retained columns\n",
    "retained_cols = transformer.get_feature_names_out(input_features=df_oil.columns)\n",
    "\n",
    "# Create a new DataFrame\n",
    "data_cleaned = pd.DataFrame(data_transformed, columns=retained_cols)\n",
    "\n",
    "print(\"\\nData after removing zero-variance column (f_4):\")\n",
    "print(data_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f_1    False\n",
       "f_2    False\n",
       "f_3    False\n",
       "f_4     True\n",
       "f_5    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_oil.nunique()==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>25.4</td>\n",
       "      <td>3.8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>22.3</td>\n",
       "      <td>4.1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>26.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24.8</td>\n",
       "      <td>3.9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>22.3</td>\n",
       "      <td>4.1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f_1   f_2  f_3  f_5\n",
       "0    1  25.4  3.8   10\n",
       "1    2  22.3  4.1   12\n",
       "2    3  26.1  3.7   10\n",
       "3    4  24.8  3.9   11\n",
       "4    2  22.3  4.1   12"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_oil.drop(columns=df_oil.columns[df_oil.nunique()==1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Remove Duplicate Rows\n",
    "\n",
    "**Logical Explanation:**\n",
    "A duplicate row is a row where all column values are identical to another existing row. Keeping these duplicates can skew analysis and lead to falsely optimistic model evaluations, as the model might be trained and tested on the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial data:\n",
      "   f_1   f_2  f_3  f_4  f_5\n",
      "0    1  25.4  3.8    0   10\n",
      "1    2  22.3  4.1    0   12\n",
      "2    3  26.1  3.7    0   10\n",
      "3    4  24.8  3.9    0   11\n",
      "4    2  22.3  4.1    0   12\n",
      "\n",
      "Number of duplicate rows: 1\n",
      "\n",
      "Data after removing duplicate rows:\n",
      "   f_1   f_2  f_3  f_4  f_5\n",
      "0    1  25.4  3.8    0   10\n",
      "1    2  22.3  4.1    0   12\n",
      "2    3  26.1  3.7    0   10\n",
      "3    4  24.8  3.9    0   11\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial data:\")\n",
    "print(df_oil)\n",
    "\n",
    "# Check for duplicate rows\n",
    "print(f\"\\nNumber of duplicate rows: {df_oil.duplicated().sum()}\")\n",
    "\n",
    "# Remove duplicate rows\n",
    "data_no_dup = df_oil.drop_duplicates()\n",
    "print(\"\\nData after removing duplicate rows:\")\n",
    "print(data_no_dup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Lab #1: Practice with full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2558</td>\n",
       "      <td>1506.09</td>\n",
       "      <td>456.63</td>\n",
       "      <td>90</td>\n",
       "      <td>6395000.0</td>\n",
       "      <td>40.88</td>\n",
       "      <td>7.89</td>\n",
       "      <td>29780.0</td>\n",
       "      <td>0.19</td>\n",
       "      <td>...</td>\n",
       "      <td>2850.00</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>763.16</td>\n",
       "      <td>135.46</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0</td>\n",
       "      <td>33243.19</td>\n",
       "      <td>65.74</td>\n",
       "      <td>7.95</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>22325</td>\n",
       "      <td>79.11</td>\n",
       "      <td>841.03</td>\n",
       "      <td>180</td>\n",
       "      <td>55812500.0</td>\n",
       "      <td>51.11</td>\n",
       "      <td>1.21</td>\n",
       "      <td>61900.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>5750.00</td>\n",
       "      <td>11500.00</td>\n",
       "      <td>9593.48</td>\n",
       "      <td>1648.80</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0</td>\n",
       "      <td>51572.04</td>\n",
       "      <td>65.73</td>\n",
       "      <td>6.26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>115</td>\n",
       "      <td>1449.85</td>\n",
       "      <td>608.43</td>\n",
       "      <td>88</td>\n",
       "      <td>287500.0</td>\n",
       "      <td>40.42</td>\n",
       "      <td>7.34</td>\n",
       "      <td>3340.0</td>\n",
       "      <td>0.18</td>\n",
       "      <td>...</td>\n",
       "      <td>1400.00</td>\n",
       "      <td>250.00</td>\n",
       "      <td>150.00</td>\n",
       "      <td>45.13</td>\n",
       "      <td>9.33</td>\n",
       "      <td>1</td>\n",
       "      <td>31692.84</td>\n",
       "      <td>65.81</td>\n",
       "      <td>7.84</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1201</td>\n",
       "      <td>1562.53</td>\n",
       "      <td>295.65</td>\n",
       "      <td>66</td>\n",
       "      <td>3002500.0</td>\n",
       "      <td>42.40</td>\n",
       "      <td>7.97</td>\n",
       "      <td>18030.0</td>\n",
       "      <td>0.19</td>\n",
       "      <td>...</td>\n",
       "      <td>6041.52</td>\n",
       "      <td>761.58</td>\n",
       "      <td>453.21</td>\n",
       "      <td>144.97</td>\n",
       "      <td>13.33</td>\n",
       "      <td>1</td>\n",
       "      <td>37696.21</td>\n",
       "      <td>65.67</td>\n",
       "      <td>8.07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>312</td>\n",
       "      <td>950.27</td>\n",
       "      <td>440.86</td>\n",
       "      <td>37</td>\n",
       "      <td>780000.0</td>\n",
       "      <td>41.43</td>\n",
       "      <td>7.03</td>\n",
       "      <td>3350.0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>1320.04</td>\n",
       "      <td>710.63</td>\n",
       "      <td>512.54</td>\n",
       "      <td>109.16</td>\n",
       "      <td>2.58</td>\n",
       "      <td>0</td>\n",
       "      <td>29038.17</td>\n",
       "      <td>65.66</td>\n",
       "      <td>7.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1        2       3    4           5      6     7        8     9   \\\n",
       "0   1   2558  1506.09  456.63   90   6395000.0  40.88  7.89  29780.0  0.19   \n",
       "1   2  22325    79.11  841.03  180  55812500.0  51.11  1.21  61900.0  0.02   \n",
       "2   3    115  1449.85  608.43   88    287500.0  40.42  7.34   3340.0  0.18   \n",
       "3   4   1201  1562.53  295.65   66   3002500.0  42.40  7.97  18030.0  0.19   \n",
       "4   5    312   950.27  440.86   37    780000.0  41.43  7.03   3350.0  0.17   \n",
       "\n",
       "   ...       40        41       42       43     44  45        46     47    48  \\\n",
       "0  ...  2850.00   1000.00   763.16   135.46   3.73   0  33243.19  65.74  7.95   \n",
       "1  ...  5750.00  11500.00  9593.48  1648.80   0.60   0  51572.04  65.73  6.26   \n",
       "2  ...  1400.00    250.00   150.00    45.13   9.33   1  31692.84  65.81  7.84   \n",
       "3  ...  6041.52    761.58   453.21   144.97  13.33   1  37696.21  65.67  8.07   \n",
       "4  ...  1320.04    710.63   512.54   109.16   2.58   0  29038.17  65.66  7.35   \n",
       "\n",
       "   49  \n",
       "0   1  \n",
       "1   0  \n",
       "2   1  \n",
       "3   1  \n",
       "4   0  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_lab1 = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
    "df_lab1 = pd.read_csv(url_lab1, header=None)\n",
    "df_lab1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Identify and Remove Outliers (Chapter 6)\n",
    "\n",
    "We will use the `housing` dataset for illustration. This dataset contains information about house prices, and extremely large or small values could be outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial housing data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.90000</td>\n",
       "      <td>80</td>\n",
       "      <td>20.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.600</td>\n",
       "      <td>12.000</td>\n",
       "      <td>90.0</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>5</td>\n",
       "      <td>666</td>\n",
       "      <td>20.0</td>\n",
       "      <td>350.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM  ZN  INDUS  CHAS    NOX      RM   AGE     DIS  RAD  TAX  PTRATIO  \\\n",
       "0  0.00632  18   2.31     0  0.538   6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0   7.07     0  0.469   6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0   7.07     0  0.469   7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0   2.18     0  0.458   6.998  45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0   2.18     0  0.458   7.147  54.2  6.0622    3  222     18.7   \n",
       "5  0.90000  80  20.00     0  0.600  12.000  90.0  2.0000    5  666     20.0   \n",
       "\n",
       "        B  LSTAT   MEDV  \n",
       "0  396.90   4.98   24.0  \n",
       "1  396.90   9.14   21.6  \n",
       "2  392.83   4.03   34.7  \n",
       "3  394.63   2.94   33.4  \n",
       "4  396.90   5.33   36.2  \n",
       "5  350.00  30.00  500.0  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create sample housing data\n",
    "csv_housing = '''CRIM,ZN,INDUS,CHAS,NOX,RM,AGE,DIS,RAD,TAX,PTRATIO,B,LSTAT,MEDV\n",
    "0.00632,18,2.31,0,0.538,6.575,65.2,4.09,1,296,15.3,396.9,4.98,24\n",
    "0.02731,0,7.07,0,0.469,6.421,78.9,4.9671,2,242,17.8,396.9,9.14,21.6\n",
    "0.02729,0,7.07,0,0.469,7.185,61.1,4.9671,2,242,17.8,392.83,4.03,34.7\n",
    "0.03237,0,2.18,0,0.458,6.998,45.8,6.0622,3,222,18.7,394.63,2.94,33.4\n",
    "0.06905,0,2.18,0,0.458,7.147,54.2,6.0622,3,222,18.7,396.9,5.33,36.2\n",
    "0.9,80,20,0,0.6,12,90,2,5,666,20,350,30,500''' # Row that may contain outliers\n",
    "\n",
    "df_housing = pd.read_csv(io.StringIO(csv_housing))\n",
    "print(\"Initial housing data:\")\n",
    "df_housing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Standard Deviation Method\n",
    "\n",
    "**Mathematical Explanation:**\n",
    "This method assumes the data has a Gaussian (bell-shaped) distribution. We define a boundary based on the mean (μ) and standard deviation (σ).\n",
    "- **Upper bound:** $ \\mu + 3 \\times \\sigma $\n",
    "- **Lower bound:** $ \\mu - 3 \\times \\sigma $\n",
    "Any data point falling outside this range is considered an outlier. The number 3 is common but can be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 outliers.\n",
      "     RM   MEDV\n",
      "5  12.0  500.0\n",
      "\n",
      "Original data size: 6\n",
      "Data size after cleaning: 5\n"
     ]
    }
   ],
   "source": [
    "# Consider the MEDV column (house price)\n",
    "data_col = df_housing['MEDV']\n",
    "\n",
    "# Calculate the limits\n",
    "mean, std = data_col.mean(), data_col.std()\n",
    "cut_off = std * 2 # Using 2 std to make outliers more visible in this small dataset\n",
    "lower, upper = mean - cut_off, mean + cut_off\n",
    "\n",
    "# Identify outliers\n",
    "outliers = df_housing[(data_col < lower) | (data_col > upper)]\n",
    "print(f\"Found {len(outliers)} outliers.\")\n",
    "print(outliers[['RM', 'MEDV']])\n",
    "\n",
    "# Remove outliers\n",
    "data_cleaned_outlier = df_housing[(data_col >= lower) & (data_col <= upper)]\n",
    "print(f\"\\nOriginal data size: {len(df_housing)}\")\n",
    "print(f\"Data size after cleaning: {len(data_cleaned_outlier)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Interquartile Range (IQR) Method\n",
    "\n",
    "**Mathematical Explanation:**\n",
    "This method does not require the data to have a Gaussian distribution. It is based on quartiles:\n",
    "- **Q1:** The first quartile (25% of data is below it).\n",
    "- **Q3:** The third quartile (75% of data is below it).\n",
    "- **IQR (Interquartile Range):** $ Q3 - Q1 $\n",
    "\n",
    "The boundaries are defined as:\n",
    "- **Upper bound:** $ Q3 + 1.5 \\times IQR $\n",
    "- **Lower bound:** $ Q1 - 1.5 \\times IQR $\n",
    "Points falling outside this range are considered outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 outliers using IQR.\n",
      "     RM   MEDV\n",
      "5  12.0  500.0\n"
     ]
    }
   ],
   "source": [
    "# Reusing the MEDV column\n",
    "data_col = df_housing['MEDV']\n",
    "\n",
    "# Calculate Q1, Q3, and IQR\n",
    "Q1 = data_col.quantile(0.25)\n",
    "Q3 = data_col.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Calculate the limits\n",
    "lower_iqr, upper_iqr = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "\n",
    "# Identify outliers\n",
    "outliers_iqr = df_housing[(data_col < lower_iqr) | (data_col > upper_iqr)]\n",
    "print(f\"Found {len(outliers_iqr)} outliers using IQR.\")\n",
    "print(outliers_iqr[['RM', 'MEDV']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = [\n",
    "    \"CRIM\",    # Tội phạm bình quân đầu người theo thị trấn\n",
    "    \"ZN\",      # Tỷ lệ đất ở > 25,000 sq.ft\n",
    "    \"INDUS\",   # Tỷ lệ diện tích cho doanh nghiệp phi bán lẻ\n",
    "    \"CHAS\",    # Biến giả sông Charles (=1 nếu gần sông, 0 nếu không)\n",
    "    \"NOX\",     # Nồng độ oxit nitơ (phần triệu)\n",
    "    \"RM\",      # Số phòng trung bình mỗi căn hộ\n",
    "    \"AGE\",     # % căn hộ xây dựng trước 1940\n",
    "    \"DIS\",     # Khoảng cách bình quân đến 5 trung tâm việc làm\n",
    "    \"RAD\",     # Chỉ số khả năng tiếp cận đường cao tốc\n",
    "    \"TAX\",     # Thuế bất động sản\n",
    "    \"PTRATIO\", # Tỷ lệ học sinh/giáo viên\n",
    "    \"B\",       # 1000(Bk - 0.63)^2, với Bk % dân da đen\n",
    "    \"LSTAT\",   # % dân có địa vị kinh tế xã hội thấp\n",
    "    \"MEDV\"     # Giá trị trung vị của nhà (ngàn USD)\n",
    "]\n",
    "url_lab2 = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv\"\n",
    "df_lab2 = pd.read_csv(url_lab2, header=None, names=column_names)\n",
    "df_lab2\n",
    "df_lab2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Automatic Outlier Detection (Local Outlier Factor)\n",
    "\n",
    "**Logical Explanation:**\n",
    "Local Outlier Factor (LOF) does not look at the entire dataset but compares the density of a data point to the density of its \"neighbors\".\n",
    "- If a point has a significantly lower density than its neighbors, it is considered to be in a \"sparse\" region and is likely an outlier.\n",
    "- The algorithm assigns a score to each sample. The higher the score, the more likely it is an outlier. In scikit-learn, outliers are typically labeled as -1.\n",
    "\n",
    "--- \n",
    "### Details on `sklearn.neighbors.LocalOutlierFactor`\n",
    "This is an unsupervised learning model for anomaly detection.\n",
    "\n",
    "**Usage:**\n",
    "1. Initialize a `LocalOutlierFactor` object.\n",
    "2. Use the `.fit_predict()` method on the training data. This method will return an array: `1` for normal points (inliers) and `-1` for outliers. \n",
    "3. `.predict()` on the test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Important Parameters:**\n",
    "- `n_neighbors` (int, default=20): The number of neighbors used to calculate the local density. This is the most important parameter to tune.\n",
    "- `contamination` (float, default='auto'): The expected proportion of outliers in the dataset (e.g., 0.1 for 10%). This parameter affects the model's decision threshold. The default 'auto' will determine the threshold based on the original algorithm's publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required class\n",
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers found: 0\n",
      "Data size after removing outliers: (6, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CODE\\env-teaching\\Lib\\site-packages\\sklearn\\neighbors\\_lof.py:282: UserWarning: n_neighbors (20) is greater than the total number of samples (6). n_neighbors will be set to (n_samples - 1) for estimation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Use LOF on the entire housing dataset\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(df_housing)\n",
    "\n",
    "# Filter out the outliers (LOF labels outliers as -1)\n",
    "mask = yhat != -1\n",
    "print(f\"Number of outliers found: {sum(yhat == -1)}\")\n",
    "print(f\"Data size after removing outliers: {df_housing[mask].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handling Missing Data (Chapters 7-10)\n",
    "\n",
    "We will use a simplified version of the `horse-colic` dataset for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial horse-colic data:\n",
      "   hospital_number  rectal_temp  pulse  respiratory_rate  pain  outcome\n",
      "0           530101         38.5     66              28.0   3.0        2\n",
      "1           534817         39.2     88              20.0   NaN        1\n",
      "2           530334         38.3     40               NaN   3.0        1\n",
      "3           529048         39.1    164              84.0   4.0        2\n",
      "4           526254          NaN     72               NaN   2.0        1\n"
     ]
    }
   ],
   "source": [
    "# Create sample horse-colic data\n",
    "csv_horse = '''hospital_number,rectal_temp,pulse,respiratory_rate,pain,outcome\n",
    "530101,38.5,66,28,3,2\n",
    "534817,39.2,88,20,?,1\n",
    "530334,38.3,40,?,3,1\n",
    "529048,39.1,164,84,4,2\n",
    "526254,?,72,?,2,1'''\n",
    "\n",
    "# Read data, considering '?' as a missing value\n",
    "df_horse = pd.read_csv(io.StringIO(csv_horse), na_values='?')\n",
    "print(\"Initial horse-colic data:\")\n",
    "print(df_horse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Marking and Removing Missing Data (Chapter 7)\n",
    "\n",
    "**Logical Explanation:**\n",
    "This is the simplest approach. If a row contains at least one missing value, we remove the entire row. The advantage is speed and simplicity, but the disadvantage is the potential loss of a large amount of valuable data if missing values are scattered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial data:\n",
      "   hospital_number  rectal_temp  pulse  respiratory_rate  pain  outcome\n",
      "0           530101         38.5     66              28.0   3.0        2\n",
      "1           534817         39.2     88              20.0   NaN        1\n",
      "2           530334         38.3     40               NaN   3.0        1\n",
      "3           529048         39.1    164              84.0   4.0        2\n",
      "4           526254          NaN     72               NaN   2.0        1\n",
      "\n",
      "Number of missing values per column:\n",
      "hospital_number     0\n",
      "rectal_temp         1\n",
      "pulse               0\n",
      "respiratory_rate    2\n",
      "pain                1\n",
      "outcome             0\n",
      "dtype: int64\n",
      "\n",
      "Data after dropping missing rows:\n",
      "   hospital_number  rectal_temp  pulse  respiratory_rate  pain  outcome\n",
      "0           530101         38.5     66              28.0   3.0        2\n",
      "3           529048         39.1    164              84.0   4.0        2\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial data:\")\n",
    "print(df_horse)\n",
    "\n",
    "# Check the number of missing values\n",
    "print(\"\\nNumber of missing values per column:\")\n",
    "print(df_horse.isnull().sum())\n",
    "\n",
    "# Remove rows with missing values\n",
    "data_dropped = df_horse.dropna()\n",
    "print(\"\\nData after dropping missing rows:\")\n",
    "print(data_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Statistical Imputation (Chapter 8)\n",
    "\n",
    "**Mathematical Explanation:**\n",
    "We replace a missing value (NaN) in a column with a statistical value calculated from the remaining valid values in that column.\n",
    "- **Mean:** $ \\bar{x} = \\frac{\\sum x_i}{n} $. Suitable for numerical data without outliers.\n",
    "- **Median:** The middle value of the column after sorting. Suitable for numerical data with outliers.\n",
    "- **Most Frequent (Mode):** The value that appears most often. Suitable for categorical data.\n",
    "\n",
    "--- \n",
    "### Details on `sklearn.impute.SimpleImputer`\n",
    "This is a transformer class for handling missing values. It provides basic imputation strategies.\n",
    "\n",
    "**Usage:**\n",
    "1. Initialize a `SimpleImputer` object, specifying the imputation strategy via the `strategy` parameter.\n",
    "2. Use `.fit_transform()` on the data.\n",
    "\n",
    "**Important Parameters:**\n",
    "- `strategy` (string, default='mean'): The imputation strategy. Possible values are `'mean'`, `'median'`, `'most_frequent'`, or `'constant'`. \n",
    "- `fill_value` (string or number, default=None): When `strategy='constant'`, this parameter is used to specify the value to be imputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required class\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after imputing with the mean:\n",
      "   hospital_number  rectal_temp  pulse  respiratory_rate  pain  outcome\n",
      "0         530101.0       38.500   66.0              28.0   3.0      2.0\n",
      "1         534817.0       39.200   88.0              20.0   3.0      1.0\n",
      "2         530334.0       38.300   40.0              44.0   3.0      1.0\n",
      "3         529048.0       39.100  164.0              84.0   4.0      2.0\n",
      "4         526254.0       38.775   72.0              44.0   2.0      1.0\n"
     ]
    }
   ],
   "source": [
    "# Use SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data_imputed_mean = imputer.fit_transform(df_horse)\n",
    "\n",
    "print(\"Data after imputing with the mean:\")\n",
    "print(pd.DataFrame(data_imputed_mean, columns=df_horse.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. KNN Imputation (Chapter 9)\n",
    "\n",
    "**Logical Explanation:**\n",
    "K-Nearest Neighbors (KNN) Imputation works on the idea that a data point can be predicted by the data points closest to it.\n",
    "1. To impute a missing value, the algorithm finds the `k` nearest rows (neighbors) to the row with the missing value. \"Nearest\" is measured by distance (e.g., Euclidean distance) based on the columns that have values.\n",
    "2. The missing value is then estimated by taking the average (or weighted average) of the values from those `k` neighbors in the same column.\n",
    "\n",
    "--- \n",
    "### Details on `sklearn.impute.KNNImputer`\n",
    "This transformer class imputes missing values using the k-Nearest Neighbors method.\n",
    "\n",
    "**Usage:**\n",
    "1. Initialize `KNNImputer` with the desired number of neighbors `n_neighbors`.\n",
    "2. Use `.fit_transform()` to learn from the data and perform the imputation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Important Parameters:**\n",
    "- `n_neighbors` (int, default=5): The number of neighbors to use for imputation.\n",
    "- `weights` (string, default='uniform'): The weight function used in prediction. `'uniform'` means all neighbors are weighted equally. `'distance'` means that closer neighbors will have a greater influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required class\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after KNN imputation:\n",
      "   hospital_number  rectal_temp  pulse  respiratory_rate  pain  outcome\n",
      "0         530101.0         38.5   66.0              28.0   3.0      2.0\n",
      "1         534817.0         39.2   88.0              20.0   3.0      1.0\n",
      "2         530334.0         38.3   40.0              56.0   3.0      1.0\n",
      "3         529048.0         39.1  164.0              84.0   4.0      2.0\n",
      "4         526254.0         38.8   72.0              56.0   2.0      1.0\n"
     ]
    }
   ],
   "source": [
    "# Use KNNImputer\n",
    "knn_imputer = KNNImputer(n_neighbors=2, weights='uniform')\n",
    "data_imputed_knn = knn_imputer.fit_transform(df_horse)\n",
    "\n",
    "print(\"Data after KNN imputation:\")\n",
    "print(pd.DataFrame(data_imputed_knn, columns=df_horse.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Iterative Imputation (Chapter 10)\n",
    "\n",
    "**Logical Explanation:**\n",
    "This is a more sophisticated method that treats imputation as a machine learning problem.\n",
    "1. For a column `C` with missing values, the algorithm treats column `C` as the target variable (`y`) and all other columns as input features (`X`).\n",
    "2. It builds a regression model (e.g., Linear Regression) on the rows without missing data in column `C` to learn the relationship `y = f(X)`.\n",
    "3. This model is then used to predict and fill in the missing values in column `C`.\n",
    "4. This process is repeated for all columns with missing values, and the entire cycle is repeated multiple times (iteratively). In each iteration, the imputed values from the previous step are used to improve the next predictions until the results converge.\n",
    "\n",
    "--- \n",
    "### Details on `sklearn.impute.IterativeImputer`\n",
    "This is a multivariate transformer class that imputes missing values by modeling each feature as a function of other features.\n",
    "\n",
    "**Usage:**\n",
    "1. Initialize `IterativeImputer`. The internal regression model can be customized via the `estimator` parameter.\n",
    "2. Use `.fit_transform()` to perform the iterative imputation process.\n",
    "\n",
    "**Important Parameters:**\n",
    "- `estimator` (object, default=BayesianRidge()): The regression model used to predict missing values. Other models like `RandomForestRegressor` can be used.\n",
    "- `max_iter` (int, default=10): The maximum number of imputation rounds.\n",
    "- `random_state` (int): To ensure reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required classes\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after iterative imputation:\n",
      "   hospital_number  rectal_temp  pulse  respiratory_rate      pain  outcome\n",
      "0         530101.0     38.50000   66.0         28.000000  3.000000      2.0\n",
      "1         534817.0     39.20000   88.0         20.000000  4.576976      1.0\n",
      "2         530334.0     38.30000   40.0         13.338398  3.000000      1.0\n",
      "3         529048.0     39.10000  164.0         84.000000  4.000000      2.0\n",
      "4         526254.0     38.10825   72.0         47.142432  2.000000      1.0\n"
     ]
    }
   ],
   "source": [
    "# Use IterativeImputer\n",
    "iter_imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "data_imputed_iter = iter_imputer.fit_transform(df_horse)\n",
    "\n",
    "print(\"Data after iterative imputation:\")\n",
    "print(pd.DataFrame(data_imputed_iter, columns=df_horse.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 lab #3: Practice with full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surgery</th>\n",
       "      <th>age</th>\n",
       "      <th>hospital_number</th>\n",
       "      <th>rectal_temp</th>\n",
       "      <th>pulse</th>\n",
       "      <th>respiratory_rate</th>\n",
       "      <th>temp_extremities</th>\n",
       "      <th>peripheral_pulse</th>\n",
       "      <th>mucous_membrane</th>\n",
       "      <th>capillary_refill</th>\n",
       "      <th>...</th>\n",
       "      <th>packed_cell_volume</th>\n",
       "      <th>total_protein</th>\n",
       "      <th>abdomocentesis_appearance</th>\n",
       "      <th>abdomocentesis_total_protein</th>\n",
       "      <th>outcome</th>\n",
       "      <th>surgical_lesion</th>\n",
       "      <th>lesion_1</th>\n",
       "      <th>lesion_2</th>\n",
       "      <th>lesion_3</th>\n",
       "      <th>lesion_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>530101</td>\n",
       "      <td>38.5</td>\n",
       "      <td>66.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>8.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>11300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>534817</td>\n",
       "      <td>39.2</td>\n",
       "      <td>88.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2208</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>530334</td>\n",
       "      <td>38.3</td>\n",
       "      <td>40.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>5290409</td>\n",
       "      <td>39.1</td>\n",
       "      <td>164.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>48.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2208</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>530255</td>\n",
       "      <td>37.3</td>\n",
       "      <td>104.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>74.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   surgery  age  hospital_number  rectal_temp  pulse  respiratory_rate  \\\n",
       "0      2.0    1           530101         38.5   66.0              28.0   \n",
       "1      1.0    1           534817         39.2   88.0              20.0   \n",
       "2      2.0    1           530334         38.3   40.0              24.0   \n",
       "3      1.0    9          5290409         39.1  164.0              84.0   \n",
       "4      2.0    1           530255         37.3  104.0              35.0   \n",
       "\n",
       "   temp_extremities  peripheral_pulse  mucous_membrane  capillary_refill  ...  \\\n",
       "0               3.0               3.0              NaN               2.0  ...   \n",
       "1               NaN               NaN              4.0               1.0  ...   \n",
       "2               1.0               1.0              3.0               1.0  ...   \n",
       "3               4.0               1.0              6.0               2.0  ...   \n",
       "4               NaN               NaN              6.0               2.0  ...   \n",
       "\n",
       "   packed_cell_volume  total_protein  abdomocentesis_appearance  \\\n",
       "0                45.0            8.4                        NaN   \n",
       "1                50.0           85.0                        2.0   \n",
       "2                33.0            6.7                        NaN   \n",
       "3                48.0            7.2                        3.0   \n",
       "4                74.0            7.4                        NaN   \n",
       "\n",
       "   abdomocentesis_total_protein  outcome  surgical_lesion  lesion_1  lesion_2  \\\n",
       "0                           NaN      2.0                2     11300         0   \n",
       "1                           2.0      3.0                2      2208         0   \n",
       "2                           NaN      1.0                2         0         0   \n",
       "3                           5.3      2.0                1      2208         0   \n",
       "4                           NaN      2.0                2      4300         0   \n",
       "\n",
       "   lesion_3  lesion_4  \n",
       "0         0         2  \n",
       "1         0         2  \n",
       "2         0         1  \n",
       "3         0         1  \n",
       "4         0         2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_lab3 = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv\"\n",
    "\n",
    "col_names = [\n",
    "    \"surgery\", \"age\", \"hospital_number\", \"rectal_temp\", \"pulse\",\n",
    "    \"respiratory_rate\", \"temp_extremities\", \"peripheral_pulse\",\n",
    "    \"mucous_membrane\", \"capillary_refill\", \"pain\", \"peristalsis\",\n",
    "    \"abdominal_distension\", \"nasogastric_tube\", \"nasogastric_reflux\",\n",
    "    \"nasogastric_reflux_ph\", \"rectal_exam_feces\", \"abdomen\",\n",
    "    \"packed_cell_volume\", \"total_protein\", \"abdomocentesis_appearance\",\n",
    "    \"abdomocentesis_total_protein\", \"outcome\", \"surgical_lesion\",\n",
    "    \"lesion_1\", \"lesion_2\", \"lesion_3\", \"lesion_4\"\n",
    "]\n",
    "\n",
    "df_lab3 = pd.read_csv(url_lab3, header=None, names=col_names, na_values=\"?\")\n",
    "\n",
    "df_lab3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Automating the Workflow with Pipelines (Best Practice)\n",
    "\n",
    "In practice, data cleaning steps (like imputation) and model training should not be separate. Scikit-learn's `Pipeline` is a powerful tool for combining multiple processing and modeling steps into a single workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Why are Pipelines Important?\n",
    "\n",
    "**Logical Explanation:**\n",
    "Imagine you are imputing missing data with the mean. The wrong way to do it is to calculate the mean on the *entire* dataset, fill in the missing values, and *then* split into training and test sets. By doing this, information from the test set (specifically, its values contributing to the mean) has \"leaked\" into the training process. This leads to a falsely optimistic performance on your test set, but the model will perform poorly on real-world data.\n",
    "\n",
    "**A `Pipeline` solves this by:**\n",
    "1.  **Encapsulating the process:** It combines steps like `SimpleImputer` and `RandomForestClassifier` into a single object.\n",
    "2.  **Preventing data leakage:** When you call `pipeline.fit(X_train, y_train)`, it will only \"learn\" (fit) the preprocessing steps (e.g., calculate the mean) on `X_train`. When you call `pipeline.predict(X_test)`, it will apply the learned transformation to `X_test` without re-learning. This accurately simulates the real-world workflow and ensures the integrity of the model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Practical Example with a Pipeline\n",
    "\n",
    "In this example, we will build a complete workflow: impute missing data and then train a classification model. We will explore the components used in more detail.\n",
    "\n",
    "--- \n",
    "### Details on Components within the Pipeline\n",
    "\n",
    "**1. `sklearn.model_selection.train_test_split`**\n",
    "This is a utility function to split data into training and testing sets, ensuring that model evaluation is objective.\n",
    "- **Usage:** `train_test_split(X, y, test_size=0.3, random_state=42)`\n",
    "- **Important Parameters:**\n",
    "  - `test_size` (float, default=0.25): The proportion of the dataset to allocate to the test set.\n",
    "  - `random_state` (int): Ensures that the data split is the same every time the code is run, making results reproducible.\n",
    "  - `stratify` (array-like, default=None): If provided (usually `y`), the data is split in a stratified fashion, preserving the same proportion of classes in both the train and test sets. Very useful for imbalanced classification problems.\n",
    "\n",
    "**2. `sklearn.ensemble.RandomForestClassifier`**\n",
    "A powerful classification model from the \"ensemble\" family of algorithms. It builds multiple decision trees and combines their results to make a final prediction, which increases stability and accuracy.\n",
    "- **Usage:** Initialize a `RandomForestClassifier` object, then use `.fit()` to train and `.predict()` to make predictions.\n",
    "- **Important Parameters:**\n",
    "  - `n_estimators` (int, default=100): The number of decision trees in the forest.\n",
    "  - `max_depth` (int, default=None): The maximum depth of each tree. Helps control model complexity and combat overfitting.\n",
    "  - `criterion` (string, default='gini'): The function to measure the quality of a split. Can be `'gini'` or `'entropy'`.\n",
    "\n",
    "**3. `sklearn.pipeline.Pipeline`**\n",
    "The main class for encapsulating processing steps and a model into a single object.\n",
    "- **Usage:** Initialize `Pipeline` by passing a list of tuples. Each tuple contains an identifier name (string) and an estimator object (e.g., `('imputer', SimpleImputer())`). The Pipeline has `.fit()`, `.predict()`, and `.transform()` methods.\n",
    "- **Important Parameters:**\n",
    "  - `steps` (list): The list of processing steps and the model. The final step must be an estimator (has `.fit()`). All preceding steps must be transformers (have `.fit_transform()`).\n",
    "\n",
    "**4. `sklearn.metrics.accuracy_score`**\n",
    "A function to measure model performance by calculating the percentage of correct predictions.\n",
    "- **Usage:** `accuracy_score(y_true, y_pred)`\n",
    "- **Important Parameters:**\n",
    "  - `y_true`: The ground truth labels.\n",
    "  - `y_pred`: The labels predicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary classes for this example\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer # Already imported, but good practice to have it here\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline has been trained.\n",
      "Accuracy on the test set: 0.7333\n"
     ]
    }
   ],
   "source": [
    "# 1. Prepare data from horse-colic\n",
    "# Drop rows where the target variable 'outcome' is missing\n",
    "df_lab3_clean = df_lab3.dropna(subset=['outcome'])\n",
    "X = df_lab3_clean.drop('outcome', axis=1)\n",
    "y = df_lab3_clean['outcome']\n",
    "\n",
    "# 2. Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Create the Pipeline\n",
    "# This Pipeline will consist of 2 steps:\n",
    "# 'imputer': Impute missing values with the median.\n",
    "# 'model': Train a Random Forest model.\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('model', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# 4. Train the entire Pipeline on the training set\n",
    "# Scikit-learn will automatically:\n",
    "# - Call imputer.fit_transform(X_train)\n",
    "# - Then use the result to train model.fit(X_train_transformed, y_train)\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(f\"Pipeline has been trained.\")\n",
    "\n",
    "# 5. Evaluate the Pipeline on the test set\n",
    "# Scikit-learn will automatically:\n",
    "# - Call imputer.transform(X_test) (NOT re-fitting)\n",
    "# - Then use the result to predict model.predict(X_test_transformed)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy on the test set: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "We have covered the core data cleaning techniques:\n",
    "\n",
    "- **Basic Cleaning:** Removing zero-variance features and duplicate samples.\n",
    "- **Handling Outliers:** Using statistical methods and machine learning models to identify and remove abnormal data points.\n",
    "- **Handling Missing Data:** From simple removal to sophisticated imputation techniques like KNN and Iterative Imputation.\n",
    "- **Using `Pipeline`:** This is the strongly recommended method to combine preprocessing and modeling steps, ensuring a robust, reproducible, and data-leakage-free workflow.\n",
    "\n",
    "Data cleaning is a foundational step that ensures the data fed into the model is reliable and of high quality, thereby improving the performance of the machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN-Inputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline has been trained.\n",
      "Accuracy on the test set: 0.7444\n"
     ]
    }
   ],
   "source": [
    "# 3. Create the Pipeline\n",
    "# This Pipeline will consist of 2 steps:\n",
    "# 'imputer': Impute missing values with the median.\n",
    "# 'model': Train a Random Forest model.\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', KNNImputer()),\n",
    "    ('model', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# 4. Train the entire Pipeline on the training set\n",
    "# Scikit-learn will automatically:\n",
    "# - Call imputer.fit_transform(X_train)\n",
    "# - Then use the result to train model.fit(X_train_transformed, y_train)\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(f\"Pipeline has been trained.\")\n",
    "\n",
    "# 5. Evaluate the Pipeline on the test set\n",
    "# Scikit-learn will automatically:\n",
    "# - Call imputer.transform(X_test) (NOT re-fitting)\n",
    "# - Then use the result to predict model.predict(X_test_transformed)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy on the test set: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline has been trained.\n",
      "Accuracy on the test set: 0.7333\n"
     ]
    }
   ],
   "source": [
    "# 1. Prepare data from horse-colic\n",
    "# Drop rows where the target variable 'outcome' is missing\n",
    "df_lab3_clean = df_lab3.dropna(subset=['outcome'])\n",
    "X = df_lab3_clean.drop('outcome', axis=1)\n",
    "y = df_lab3_clean['outcome']\n",
    "\n",
    "# 2. Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Create the Pipeline\n",
    "# This Pipeline will consist of 2 steps:\n",
    "# 'imputer': Impute missing values with the median.\n",
    "# 'model': Train a Random Forest model.\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', IterativeImputer()),\n",
    "    ('model', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# 4. Train the entire Pipeline on the training set\n",
    "# Scikit-learn will automatically:\n",
    "# - Call imputer.fit_transform(X_train)\n",
    "# - Then use the result to train model.fit(X_train_transformed, y_train)\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(f\"Pipeline has been trained.\")\n",
    "\n",
    "# 5. Evaluate the Pipeline on the test set\n",
    "# Scikit-learn will automatically:\n",
    "# - Call imputer.transform(X_test) (NOT re-fitting)\n",
    "# - Then use the result to predict model.predict(X_test_transformed)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy on the test set: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-teaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
