{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0b8b8e6e",
      "metadata": {},
      "source": [
        "## Lecture: A Comprehensive Machine Learning Workflow with Pipeline\n",
        "\n",
        "**Objective:** To build a complete, automated, and robust ML workflow using Scikit-learn's `Pipeline`. \n",
        "\n",
        "This lecture will guide the construction of a complete ML workflow, covering the following tools and concepts:\n",
        "1.  **Foundation:** `train_test_split` (with key parameters).\n",
        "2.  **Basic `Pipeline`:** Understanding the workflow (Numerical-Only Example).\n",
        "3.  **`ColumnTransformer`:** Handling numerical and categorical data in parallel.\n",
        "4.  **Full `Pipeline`:** Chaining all steps:\n",
        "    - **Data Cleaning** (`SimpleImputer`)\n",
        "    - **Data Transform** (`StandardScaler`, `OneHotEncoder`, `OrdinalEncoder`)\n",
        "    - **Feature Selection** (`SelectKBest`)\n",
        "    - **Modeling** (e.g., `LogisticRegression`)\n",
        "5.  **`KFold` and `cross_val_score`:** Reliably evaluating the model.\n",
        "6.  **`joblib`:** Saving and loading the trained pipeline for reuse."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf821ed6",
      "metadata": {},
      "source": [
        "### Part 1: Initialization: Load Libraries and Sample Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "4fe719b0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample data created:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>income</th>\n",
              "      <th>city</th>\n",
              "      <th>education</th>\n",
              "      <th>gender</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>25.0</td>\n",
              "      <td>50000.0</td>\n",
              "      <td>Hanoi</td>\n",
              "      <td>Bachelor</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>30.0</td>\n",
              "      <td>60000.0</td>\n",
              "      <td>HCMC</td>\n",
              "      <td>Master</td>\n",
              "      <td>Female</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>45.0</td>\n",
              "      <td>100000.0</td>\n",
              "      <td>Hanoi</td>\n",
              "      <td>PhD</td>\n",
              "      <td>Male</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>55.0</td>\n",
              "      <td>80000.0</td>\n",
              "      <td>Danang</td>\n",
              "      <td>Master</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>120000.0</td>\n",
              "      <td>HCMC</td>\n",
              "      <td>Bachelor</td>\n",
              "      <td>Male</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    age    income    city education  gender  target\n",
              "0  25.0   50000.0   Hanoi  Bachelor    Male       0\n",
              "1  30.0   60000.0    HCMC    Master  Female       1\n",
              "2  45.0  100000.0   Hanoi       PhD    Male       1\n",
              "3  55.0   80000.0  Danang    Master  Female       0\n",
              "4   NaN  120000.0    HCMC  Bachelor    Male       1"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# 1. Splitting & Evaluation\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "\n",
        "# 2. Pipeline Construction\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# 3. Preprocessing Steps\n",
        "from sklearn.impute import SimpleImputer       # Data Cleaning\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder # Data Transform\n",
        "from sklearn.feature_selection import SelectKBest, f_classif # Feature Selection\n",
        "\n",
        "# 4. Model (Modeling)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# 5. Metrics (Metrics)\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# --- Create Sample Data ---\n",
        "# We create a complex dataset: with missing values, numeric columns, categorical columns\n",
        "data = {\n",
        "    'age': [25, 30, 45, 55, np.nan, 35, 60, 65, 70, 22, 48, 52],\n",
        "    'income': [50000, 60000, 100000, 80000, 120000, 75000, np.nan, 200000, 180000, 45000, 90000, 110000],\n",
        "    'city': ['Hanoi', 'HCMC', 'Hanoi', 'Danang', 'HCMC', 'Danang', 'Hanoi', 'HCMC', 'Hanoi', 'Danang', 'HCMC', 'Hanoi'],\n",
        "    'education': ['Bachelor', 'Master', 'PhD', 'Master', 'Bachelor', 'PhD', 'Master', 'PhD', 'Bachelor', 'Bachelor', 'Master', 'PhD'],\n",
        "    'gender': ['Male', 'Female', 'Male', 'Female', 'Male', 'Female', 'Male', 'Female', 'Male', 'Male', 'Female', 'Female'],\n",
        "    'target': [0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(\"Sample data created:\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b34c8ad0",
      "metadata": {},
      "source": [
        "#### ► Review Questions (Part 1)\n",
        "\n",
        "1.  Which library is imported for building the main workflow? (`sklearn.pipeline`)\n",
        "2.  Which library is used to handle parallel processing for different data types? (`sklearn.compose`)\n",
        "3.  What is the purpose of importing `SimpleImputer`? (For data cleaning / handling missing values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d581606",
      "metadata": {},
      "source": [
        "### Part 2: `train_test_split` (The Foundation)\n",
        "\n",
        "This is the first **MANDATORY** step. We must separate the Test set from the training process to avoid **Data Leakage**. All `fit` operations (for imputers, scalers, models, etc.) must only be performed on `X_train`.\n",
        "\n",
        "**Key parameters of `train_test_split`:**\n",
        "* `X, y`: The features (X) and target (y) data.\n",
        "* `test_size=0.2`: The proportion of data to reserve for the test set (e.g., 20%).\n",
        "* `random_state=42`: A seed for the random number generator to ensure the split is *always the same* each time it's run. This is crucial for reproducibility.\n",
        "* `stratify=y`: (Very important for Classification) Ensures that the proportion of classes (e.g., 0s and 1s) in the `train` and `test` sets is the *same* as the original dataset. Useful for imbalanced data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "c0c92300",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (9, 5)\n",
            "X_test shape: (3, 5)\n",
            "y_train proportions: \n",
            "target\n",
            "1    0.555556\n",
            "0    0.444444\n",
            "Name: proportion, dtype: float64\n",
            "y_test proportions: \n",
            "target\n",
            "1    0.666667\n",
            "0    0.333333\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Split X (features) and y (target)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Perform the data split\n",
        "# Use stratify=y to ensure even distribution of classes in train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                    test_size=0.2, \n",
        "                                                    random_state=42, \n",
        "                                                    stratify=y)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train proportions: \\n{y_train.value_counts(normalize=True)}\")\n",
        "print(f\"y_test proportions: \\n{y_test.value_counts(normalize=True)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f0cbf6c",
      "metadata": {},
      "source": [
        "#### ► Review Questions (Part 2)\n",
        "\n",
        "1.  Why must we `train_test_split` *before* any preprocessing (like scaling or imputation)? (To prevent Data Leakage from the test set into the training process).\n",
        "2.  What is the purpose of the `stratify=y` parameter? (To ensure the class proportions in `y_train` and `y_test` are the same as the original `y`).\n",
        "3.  What happens if you forget to set `random_state`? (You will get a different split every time you run the code, making results not reproducible)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf7ea8dc",
      "metadata": {},
      "source": [
        "### Part 3: The Basic `Pipeline` (Numerical-Only Example)\n",
        "\n",
        "Before we handle complex, mixed data, let's understand how a `Pipeline` works in a simple case.\n",
        "\n",
        "Imagine a `Pipeline` as an assembly line. We will create one for **numerical-only data**.\n",
        "Our assembly line will have 3 steps:\n",
        "1.  **Impute:** Fill missing values (`SimpleImputer`).\n",
        "2.  **Scale:** Standardize the data (`StandardScaler`).\n",
        "3.  **Model:** Train the model (`LogisticRegression`).\n",
        "\n",
        "When we call `.fit()` on the pipeline, it will `fit_transform` on Step 1, pass the result to Step 2 to `fit_transform`, and finally pass that result to Step 3 to `fit`.\n",
        "When we call `.score()` or `.predict()`, it will only `transform` on Steps 1 & 2, preventing data leakage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "1ba85194",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Simple numerical-only pipeline accuracy: 1.0000\n",
            "This shows the basic concept. Now, how do we add the categorical columns? -> ColumnTransformer\n"
          ]
        }
      ],
      "source": [
        "# 1. Define the steps for the simple pipeline\n",
        "# This pipeline only works on numerical data\n",
        "simple_numerical_steps = [\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', LogisticRegression(random_state=42))\n",
        "]\n",
        "\n",
        "# 2. Create the simple Pipeline object\n",
        "simple_num_pipeline = Pipeline(steps=simple_numerical_steps)\n",
        "\n",
        "# 3. Select only the numerical features for this example\n",
        "numeric_features = ['age', 'income'] # From Part 1\n",
        "\n",
        "# 4. Fit the pipeline on the numerical training data\n",
        "simple_num_pipeline.fit(X_train[numeric_features], y_train)\n",
        "\n",
        "# 5. Score the pipeline on the numerical test data\n",
        "score = simple_num_pipeline.score(X_test[numeric_features], y_test)\n",
        "\n",
        "print(f\"Simple numerical-only pipeline accuracy: {score:.4f}\")\n",
        "print(\"This shows the basic concept. Now, how do we add the categorical columns? -> ColumnTransformer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34b4b804",
      "metadata": {},
      "source": [
        "#### ► Review Questions (Part 3)\n",
        "\n",
        "1.  What is the main benefit of using a `Pipeline` even for this simple case? (It bundles all steps into one object and automatically prevents data leakage. The `imputer` and `scaler` are `fit` on training data and only `transform` test data).\n",
        "2.  What is the required format for the `steps` parameter in a `Pipeline`? (A list of tuples, where each tuple is `(name, transformer_or_estimator)`).\n",
        "3.  What would happen if we tried to `.fit()` this `simple_num_pipeline` on the full `X_train`? (It would fail with an error, because `StandardScaler` cannot process string columns like 'city' or 'education')."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cf3b4cf",
      "metadata": {},
      "source": [
        "### Part 4: `ColumnTransformer` (Handling Mixed Data Types)\n",
        "\n",
        "This `simple_num_pipeline` fails on mixed data. The solution is `ColumnTransformer`.\n",
        "\n",
        "Our data has 3 types: numerical (`age`, `income`), nominal categorical (`city`, `gender`), and ordinal categorical (`education`).\n",
        "* **Numerical Data:** Needs imputation (Cleaning) and scaling (Transform).\n",
        "* **Nominal Data:** Needs imputation (Cleaning) and One-Hot encoding (Transform).\n",
        "* **Ordinal Data:** Needs imputation (Cleaning) and Ordinal encoding (Transform)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "bf4771f1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'preprocessor' created successfully with OrdinalEncoder for 'education'.\n"
          ]
        }
      ],
      "source": [
        "# 1. Define column lists\n",
        "numeric_features = ['age', 'income']\n",
        "nominal_features = ['city', 'gender'] # Nominal columns (no order)\n",
        "ordinal_features = ['education']     # Ordinal columns (has order)\n",
        "\n",
        "# 2. Create a sub-pipeline for NUMERICAL data\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    # SimpleImputer: Fills missing values (NaN)\n",
        "    # strategy='median': Fill with the median value (robust to outliers)\n",
        "    ('imputer', SimpleImputer(strategy='median')), \n",
        "    # StandardScaler: Scale the data (mean=0, std=1)\n",
        "    ('scaler', StandardScaler())                  \n",
        "])\n",
        "\n",
        "# 3. Create a sub-pipeline for CATEGORICAL (NOMINAL) data\n",
        "nominal_transformer = Pipeline(steps=[\n",
        "    # strategy='most_frequent': Fill missing values with the most common value\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')), \n",
        "    # OneHotEncoder: Convert 'Hanoi', 'HCMC' into 0/1 columns\n",
        "    # handle_unknown='ignore': If an unknown value (e.g., 'Haiphong') is seen in the test set, ignore it (all new columns will be 0)\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))   \n",
        "])\n",
        "\n",
        "# 4. CREATE SUB-PIPELINE FOR CATEGORICAL (ORDINAL) DATA - CHANGE\n",
        "# Define the order for the 'education' column\n",
        "education_order = ['Bachelor', 'Master', 'PhD']\n",
        "\n",
        "ordinal_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    # OrdinalEncoder: Convert 'Bachelor' -> 0, 'Master' -> 1, 'PhD' -> 2\n",
        "    # categories=[education_order]: Specify the desired order\n",
        "    # handle_unknown='use_encoded_value', unknown_value=-1: If an unknown value is found, assign it -1\n",
        "    ('ordinal', OrdinalEncoder(categories=[education_order], \n",
        "                                handle_unknown='use_encoded_value', \n",
        "                                unknown_value=-1)) \n",
        "])\n",
        "\n",
        "# 5. Combine with ColumnTransformer\n",
        "# ColumnTransformer takes a list of 'transformers'\n",
        "# Each transformer is a tuple: (name, sub_pipeline, list_of_columns_to_apply_to)\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat_nominal', nominal_transformer, nominal_features), # Pipeline for nominal columns\n",
        "        ('cat_ordinal', ordinal_transformer, ordinal_features)  # Pipeline for ordinal columns\n",
        "    ])\n",
        "\n",
        "print(\"'preprocessor' created successfully with OrdinalEncoder for 'education'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "406bb88d",
      "metadata": {},
      "source": [
        "#### ► Review Questions (Part 4)\n",
        "\n",
        "1.  Why can't we just use a single `Pipeline` for all columns in this example? (Because different columns need different transformations, e.g., `StandardScaler` for numbers vs. `OneHotEncoder` for text).\n",
        "2.  What is the main difference between `OneHotEncoder` (used for `city`) and `OrdinalEncoder` (used for `education`)? (`OneHotEncoder` creates new 0/1 columns and assumes no order. `OrdinalEncoder` creates one column with integer values (0, 1, 2) that represent a specific order).\n",
        "3.  What does the `handle_unknown='ignore'` parameter in `OneHotEncoder` do? (It prevents an error if the model sees a new category in the test data that it never saw in the training data)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ca8ada2",
      "metadata": {},
      "source": [
        "### Part 5: The Full `Pipeline` (Combining All Steps)\n",
        "\n",
        "Now we connect all the pieces:\n",
        "\n",
        "1.  **`preprocessor`**: (Cleaning + Transform) as defined above.\n",
        "2.  **`selector`**: (Feature Selection) `SelectKBest` to select features.\n",
        "3.  **`model`**: (Modeling) The `LogisticRegression` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "10d7b7aa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Full pipeline created.\n",
            "Accuracy on Test set (single split): 1.0000\n"
          ]
        }
      ],
      "source": [
        "# Pipeline connects processing steps into a single workflow.\n",
        "# 'steps' is a list of tuples: (step_name, transformer/estimator_object)\n",
        "full_pipeline = Pipeline(steps=[\n",
        "    # STEP 1: Cleaning + Transform (Using the ColumnTransformer)\n",
        "    ('preprocessor', preprocessor),\n",
        "    \n",
        "    # STEP 2: Feature Selection (Select features)\n",
        "    # SelectKBest: Select 'k' best features\n",
        "    # score_func=f_classif: Use f_classif (ANOVA F-test) to score features\n",
        "    # (After processing, we have 2 numeric + 5 OHE (city+gender) + 1 ordinal = 8 features)\n",
        "    ('selector', SelectKBest(score_func=f_classif, k=6)), # Select 6 of the 8 best features\n",
        "    \n",
        "    # STEP 3: Modeling\n",
        "    # LogisticRegression: The final model for prediction\n",
        "    ('model', LogisticRegression(random_state=42))\n",
        "])\n",
        "\n",
        "print(\"Full pipeline created.\")\n",
        "\n",
        "# -- Train and evaluate (test run) --\n",
        "# Fit only on X_train\n",
        "full_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict only on X_test\n",
        "y_pred = full_pipeline.predict(X_test)\n",
        "\n",
        "print(f\"Accuracy on Test set (single split): {accuracy_score(y_test, y_pred):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e58344d5",
      "metadata": {},
      "source": [
        "#### ► Review Questions (Part 5)\n",
        "\n",
        "1.  What is the purpose of the `full_pipeline` object? (To chain all steps of the ML workflow—preprocessing, selection, modeling—into a single object).\n",
        "2.  Does the order of steps in the `Pipeline` matter? (Yes, absolutely. Data must be cleaned/transformed *before* features can be selected, and features must be selected *before* the model is trained).\n",
        "3.  What does the `('selector', SelectKBest(k=6))` step do? (It selects the top 6 features that have the strongest relationship with the target variable, based on the `f_classif` score)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8caef10e",
      "metadata": {},
      "source": [
        "### Part 6: Evaluating the Pipeline with Cross-Validation\n",
        "\n",
        "To evaluate the model objectively, we don't rely on just one `train_test_split`. Instead, we use **Cross-Validation**.\n",
        "\n",
        "#### Introducing `KFold`\n",
        "`KFold` is a *data splitting strategy*. It splits the entire dataset (e.g., `X_train`, `y_train`) into `k` equal parts (called 'folds').\n",
        "How it works:\n",
        "1.  Iteration 1: Use Fold 1 as the test set (validation), and the remaining (k-1) folds as the training set.\n",
        "2.  Iteration 2: Use Fold 2 as the test set, and the remaining (k-1) folds as the training set.\n",
        "3.  ... (repeat k times)\n",
        "\n",
        "\n",
        "\n",
        "This way, every data point is used for both training and validation.\n",
        "* `n_splits=5`: Specify splitting into 5 folds.\n",
        "* `shuffle=True`: Shuffle the data before splitting. This is very important to ensure the folds are representative, avoiding cases where data is sorted (e.g., all class 0s at the top, class 1s at the bottom).\n",
        "* `random_state=42`: Ensures the shuffling (`shuffle`) is fixed, making the results reproducible.\n",
        "\n",
        "#### Introducing `cross_val_score`\n",
        "`cross_val_score` is the function that automates the `KFold` process.\n",
        "* `estimator`: This is the model or pipeline we want to evaluate (e.g., `full_pipeline`).\n",
        "* `X`, `y`: The data to perform cross-validation on.\n",
        "* `cv`: The splitting strategy. We will pass the `kfold` object we created above here.\n",
        "* `scoring='accuracy'`: The metric we want to use for evaluation (e.g., accuracy).\n",
        "\n",
        "This function will return an array of `k` scores (one score for each fold).\n",
        "\n",
        "**Important Note when using CV with Pipelines:**\n",
        "\n",
        "A single split result (like the one above) can be due to luck. **CRITICAL:** We must pass the **entire `full_pipeline`** into `cross_val_score`. DO NOT preprocess the data (e.g., `preprocessor.fit_transform(X)`) *before* passing it to CV, as this will cause data leakage between folds.\n",
        "\n",
        "`cross_val_score` will automatically `fit` the pipeline on (k-1) folds and `transform/predict` on the remaining fold, repeating k times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "39dac2e8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-Validation Scores (5-fold): [0.  1.  0.5 1.  1. ]\n",
            "Mean Accuracy: 0.7000\n",
            "Standard Deviation: 0.4000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\CODE\\env-teaching\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:111: RuntimeWarning: divide by zero encountered in divide\n",
            "  f = msb / msw\n"
          ]
        }
      ],
      "source": [
        "# We use cross-validation on the entire original X and y\n",
        "# (Or X_train, y_train if you want to tune parameters on the train set)\n",
        "# Here, we use (X, y) to get the most general evaluation\n",
        "\n",
        "# Define the K-Fold splitting strategy (e.g., 5 folds)\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Call cross_val_score with the ENTIRE pipeline\n",
        "cv_scores = cross_val_score(full_pipeline, X, y, \n",
        "                              cv=kfold, \n",
        "                              scoring='accuracy')\n",
        "\n",
        "print(f\"Cross-Validation Scores (5-fold): {cv_scores}\")\n",
        "print(f\"Mean Accuracy: {cv_scores.mean():.4f}\")\n",
        "print(f\"Standard Deviation: {cv_scores.std():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "228155d1",
      "metadata": {},
      "source": [
        "#### ► Review Questions (Part 6)\n",
        "\n",
        "1.  Why is using `cross_val_score` (e.g., with 5 folds) generally better than a single `train_test_split` for evaluating a model? (A single split might be 'lucky' or 'unlucky'. CV gives a more stable and reliable estimate of model performance by averaging 5 different splits).\n",
        "2.  What is the correct object to pass into `cross_val_score`'s `estimator` argument: the `preprocessor`, the `model`, or the `full_pipeline`? (The `full_pipeline`. This is critical to prevent data leakage during cross-validation).\n",
        "3.  What does the `Mean Accuracy` and `Standard Deviation` of the CV scores tell us? (Mean Accuracy is the average performance. Standard Deviation tells us how much the performance *varied* between folds; a low std is good and means the model is stable)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5605a99e",
      "metadata": {},
      "source": [
        "### Part 7: Train, Save, and Load the Model\n",
        "\n",
        "After being satisfied with the CV results, we train the pipeline on the **entire training dataset** (`X_train`, `y_train`) (or all of `X`, `y` if you are ready to deploy).\n",
        "\n",
        "We will save the **entire `full_pipeline` object**, not just the model. This ensures that when reloaded, all imputers, scalers, encoders, etc., are preserved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "e7f9e63f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- STARTING TRAINING AND SAVING MODEL ---\n",
            "Pipeline has been trained on X_train, y_train.\n",
            "FINAL accuracy on X_test: 1.0000\n",
            "Pipeline saved to file: final_model_pipeline.joblib\n"
          ]
        }
      ],
      "source": [
        "print(\"--- STARTING TRAINING AND SAVING MODEL ---\")\n",
        "\n",
        "# 1. Train the pipeline on the ENTIRE X_train, y_train\n",
        "full_pipeline.fit(X_train, y_train)\n",
        "print(\"Pipeline has been trained on X_train, y_train.\")\n",
        "\n",
        "# 2. Final evaluation on the Test set (unseen data)\n",
        "final_accuracy = full_pipeline.score(X_test, y_test)\n",
        "print(f\"FINAL accuracy on X_test: {final_accuracy:.4f}\")\n",
        "\n",
        "# 3. Save the pipeline\n",
        "# Use joblib.dump to 'freeze' the entire pipeline (including imputer, scaler, model...)\n",
        "model_filename = 'final_model_pipeline.joblib'\n",
        "joblib.dump(full_pipeline, model_filename)\n",
        "print(f\"Pipeline saved to file: {model_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "4366beca",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- LOADING AND USING THE MODEL ---\n",
            "Pipeline loaded successfully.\n",
            "\n",
            "New data for prediction:\n",
            "    age    income    city education  gender\n",
            "0  42.0  150000.0    HCMC       PhD    Male\n",
            "1   NaN   78000.0   Hanoi    Master  Female\n",
            "2  68.0       NaN  Danang  Bachelor    Male\n",
            "\n",
            "Prediction results (target): [1 0 1]\n",
            "Prediction probabilities (proba): \n",
            "[[0.17645397 0.82354603]\n",
            " [0.52858782 0.47141218]\n",
            " [0.40768686 0.59231314]]\n"
          ]
        }
      ],
      "source": [
        "print(\"--- LOADING AND USING THE MODEL ---\")\n",
        "\n",
        "# 1. Load the pipeline\n",
        "# Use joblib.load to restore the saved pipeline\n",
        "try:\n",
        "    loaded_pipeline = joblib.load(model_filename)\n",
        "    print(\"Pipeline loaded successfully.\")\n",
        "\n",
        "    # 2. Create new data (example)\n",
        "    # New data must have the EXACT same column structure as the original X_train\n",
        "    X_new = pd.DataFrame({\n",
        "        'age': [42, np.nan, 68],\n",
        "        'income': [150000, 78000, np.nan],\n",
        "        'city': ['HCMC', 'Hanoi', 'Danang'],\n",
        "        'education': ['PhD', 'Master', 'Bachelor'],\n",
        "        'gender': ['Male', 'Female', 'Male']\n",
        "    })\n",
        "\n",
        "    print(\"\\nNew data for prediction:\")\n",
        "    print(X_new)\n",
        "\n",
        "    # 3. Predict using the loaded pipeline\n",
        "    # The pipeline will AUTOMATICALLY: impute -> scale -> onehot -> ordinal -> select -> predict\n",
        "    new_predictions = loaded_pipeline.predict(X_new)\n",
        "    new_proba = loaded_pipeline.predict_proba(X_new)\n",
        "\n",
        "    print(f\"\\nPrediction results (target): {new_predictions}\")\n",
        "    print(f\"Prediction probabilities (proba): \\n{new_proba}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found {model_filename}. Please run the cell above first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba2fb645",
      "metadata": {},
      "source": [
        "#### ► Review Questions (Part 7)\n",
        "\n",
        "1.  Why is it better to save the `full_pipeline` object instead of just the `model` object (e.g., just the `LogisticRegression` step)? (Because the pipeline contains all the preprocessing steps. If you only save the model, you would have to manually re-apply the *exact same* imputation, scaling, and encoding steps to new data, which is error-prone. Saving the pipeline automates this).\n",
        "2.  What function from `joblib` is used to save a pipeline? What function is used to load it? (`joblib.dump()` to save, `joblib.load()` to load).\n",
        "3.  What must be true about the `X_new` data frame used for prediction? (It must have the *exact same* column names and structure as the original `X_train` data, even if it contains missing values)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68dd0195",
      "metadata": {},
      "source": [
        "### Part 8: Summary\n",
        "\n",
        "We have successfully built a comprehensive ML workflow:\n",
        "\n",
        "1.  **`train_test_split`** is always the first step to get a \"clean\" test set.\n",
        "2.  **`ColumnTransformer`** is essential for handling mixed data types (numeric, categorical).\n",
        "3.  **`Pipeline`** bundles all steps (Cleaning, Transform, Selection, Model) into a single object.\n",
        "4.  **`cross_val_score(pipeline, ...)`** is the correct way to evaluate the model, preventing data leakage during CV.\n",
        "5.  **`joblib.dump(pipeline, ...)`** saves the *entire* workflow, ensuring consistency when predicting on new data."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
